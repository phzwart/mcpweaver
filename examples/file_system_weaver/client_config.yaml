# Example Client Configuration
# This shows how to configure the client for any MCP server

llm:
  model: "phi3:mini"
  provider: "ollama"     # ollama, openai, anthropic, etc.
  api_url: "http://localhost:11434/api/generate"  # API endpoint for the provider
  timeout: 30  # Timeout in seconds for LLM requests
  options:
    temperature: 0.1
    top_p: 0.9
  # JSON Schema for structured output (Ollama native support)
  json_schema:
    type: "object"
    properties:
      tools:
        type: "array"
        items:
          type: "string"
          enum: ["list_files", "read_file", "count_lines", "get_file_info", "search_files"]
        description: "List of tool names to use"
      arguments:
        type: "object"
        properties:
          list_files:
            type: "object"
            properties:
              directory:
                type: "string"
                default: "."
              pattern:
                type: "string"
                default: "*"
          read_file:
            type: "object"
            properties:
              file_path:
                type: "string"
              max_lines:
                type: "integer"
                default: 50
          count_lines:
            type: "object"
            properties:
              file_path:
                type: "string"
          get_file_info:
            type: "object"
            properties:
              file_path:
                type: "string"
          search_files:
            type: "object"
            properties:
              directory:
                type: "string"
                default: "."
              pattern:
                type: "string"
                default: "*"
              content_search:
                type: "string"
        additionalProperties: false
        description: "Arguments for each tool"
    required: ["tools", "arguments"]
  
mcp_server:
  host: "localhost"
  port: 8080
  timeout: 10  # Timeout in seconds for MCP server requests

# Behavior and Prompt Instructions
behavior:
  role: "MCP Assistant"
  personality: "Helpful, analytical, and thorough"
  
  instructions: |
    You are an MCP assistant that helps users explore and analyze data.
    
    When users ask questions:
    1. Use the appropriate tools based on the query
    2. Build logical sequences of tool calls
    3. Provide clear explanations of what you're doing and why
    4. Follow the configured workflow patterns
    
    Always explain your reasoning and what tools you're using.

# Tool Selection Rules (LLM will decide based on query)
tool_selection:
  # Available tools for LLM to choose from
  list_files:
    description: "List files in a directory"
    
  read_file:
    description: "Read file contents"
    
  count_lines:
    description: "Count lines in a file"
    
  get_file_info:
    description: "Get file information"
    
  search_files:
    description: "Search for files"

# Default behavior when no specific tools are requested
default_behavior:
  show_tools_info: true
  suggest_common_queries: true

# Tool Usage Guidelines with Keywords
tools:
  list_files:
    description: "List files in a directory"
    keywords: ["list", "files", "directory", "folder", "ls", "dir"]
    when_to_use: "Use to see what files are available in a directory"
    
  read_file:
    description: "Read file contents"
    keywords: ["read", "file", "content", "view", "show", "cat"]
    when_to_use: "Use to read and display file contents"
    
  count_lines:
    description: "Count lines in a file"
    keywords: ["count", "lines", "wc", "length", "size"]
    when_to_use: "Use to count lines, empty lines, and code lines in a file"
    
  get_file_info:
    description: "Get file information"
    keywords: ["info", "information", "details", "stat", "metadata"]
    when_to_use: "Use to get detailed information about a file"
    
  search_files:
    description: "Search for files"
    keywords: ["search", "find", "grep", "pattern", "match"]
    when_to_use: "Use to search for files by pattern or content"

# Prompt Templates
prompts:
  system_prompt: |
    You are an MCP assistant. You have access to tools through an MCP server.
    
    Your job is to help users explore and analyze data. Always:
    - Start by checking system health
    - Use appropriate tools to answer questions
    - Explain what you're doing and why
    - Provide clear, helpful responses
    
  user_prompt_template: |
    User Query: {query}
    
    Available Tools:
    - list_files: List files in a directory
    - read_file: Read file contents
    - count_lines: Count lines in a file
    - get_file_info: Get file information
    - search_files: Search for files
    
    Please use the appropriate tools to answer the user's query.

# Response Format Configuration
response_format:
  include_tool_calls: true
  explain_reasoning: true
  provide_summary: true
  suggest_next_steps: true

# Summary Logic (configurable per tool)
summary_logic:
  list_files:
    success_message: "‚úÖ Found files in directory"
    failure_message: "üì≠ No files found in directory"
  
  read_file:
    success_message: "‚úÖ File read successfully"
    failure_message: "‚ùå Could not read file"

# Next Steps Suggestions
next_steps:
  - "Ask me to read a specific file"
  - "Request information about any file"
  - "Ask me to count lines in a file"
  - "Request to search for files with specific patterns"

# Response Templates
response_templates:
  tools_info_header: "üõ†Ô∏è **Available Tools:**"
  reasoning_header: "ü§î **My Reasoning:**"
  tool_results_header: "üîß **Tool Results:**"
  summary_header: "üìä **Summary:**"
  next_steps_header: "üí° **Next Steps:**"
  next_steps_default:
    - "Ask me to use any of these tools"
    - "Request specific file operations"
    - "Ask for help with data analysis"

# Argument Extraction Rules
argument_extraction:
  read_file:
    file_path:
      from_tool: "list_files"
      field: "path"
      type: "first"  # random, first, direct
  
  count_lines:
    file_path:
      from_tool: "list_files"
      field: "path"
      type: "first"
  
  get_file_info:
    file_path:
      from_tool: "list_files"
      field: "path"
      type: "first"

# LLM Reasoning Configuration
llm_reasoning:
  system_prompt_template: |
    You are an AI assistant that helps users with file system operations.

    Available tools:
    {tools}

    Your task is to:
    1. Understand what the user wants to do
    2. Select the appropriate tool(s) to use
    3. Extract any required arguments from the user's query

    Respond with a JSON object containing:
    - "tools": Array of tool names to use
    - "arguments": Object with arguments for each tool

    Examples:
    - "list files" ‚Üí {{"tools": ["list_files"], "arguments": {{}}}}
    - "read README.md" ‚Üí {{"tools": ["read_file"], "arguments": {{"read_file": {{"file_path": "README.md"}}}}}}
    - "count lines in pyproject.toml" ‚Üí {{"tools": ["count_lines"], "arguments": {{"count_lines": {{"file_path": "pyproject.toml"}}}}}}
    - "search for markdown files" ‚Üí {{"tools": ["search_files"], "arguments": {{"search_files": {{"pattern": "*.md"}}}}}}
    - "read code of conduct" ‚Üí {{"tools": ["read_file"], "arguments": {{"read_file": {{"file_path": "CODE_OF_CONDUCT.md"}}}}}}

  user_prompt_template: "User query: {query}"
  json_extraction_regex: r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'  # More precise JSON extraction
